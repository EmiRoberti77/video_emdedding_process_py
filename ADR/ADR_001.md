# Video Embedding Agent

## Initial code design

```python
# üìÅ video_to_embedding_agent
# A starter pipeline to extract keyframes, transcribe speech, and embed video content

import os
import cv2
import whisper
import openai
import torch
import tempfile
import ffmpeg
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from openai import OpenAI

# Set your API key
openai.api_key = os.getenv("OPENAI_API_KEY")

# Paths
VIDEO_PATH = "sample.mp4"
FRAME_OUTPUT_DIR = "frames"
AUDIO_FILE = "audio.mp3"

# Create output folder
os.makedirs(FRAME_OUTPUT_DIR, exist_ok=True)

# --- STEP 1: Extract Keyframes Every N Seconds ---
def extract_frames(video_path, interval_sec=5):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    count = 0
    saved = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if int(count % (fps * interval_sec)) == 0:
            fname = os.path.join(FRAME_OUTPUT_DIR, f"frame_{saved:04}.jpg")
            cv2.imwrite(fname, frame)
            saved += 1
        count += 1
    cap.release()

# --- STEP 2: Extract Audio and Transcribe ---
def extract_audio(video_path, audio_path):
    ffmpeg.input(video_path).output(audio_path).run(overwrite_output=True)

def transcribe_audio(audio_path):
    model = whisper.load_model("base")
    result = model.transcribe(audio_path)
    return result["text"]

# --- STEP 3: Embed Keyframes using CLIP ---
def embed_frames():
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    embeddings = []

    for fname in sorted(os.listdir(FRAME_OUTPUT_DIR)):
        if not fname.endswith(".jpg"):
            continue
        img_path = os.path.join(FRAME_OUTPUT_DIR, fname)
        image = Image.open(img_path).convert("RGB")
        inputs = processor(images=image, return_tensors="pt")
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
        embeddings.append((fname, image_features.numpy()))
    return embeddings

# --- STEP 4: Generate Context Embedding from Transcript ---
def get_text_embedding(text):
    response = openai.Embedding.create(
        model="text-embedding-3-small",
        input=text
    )
    return response['data'][0]['embedding']

# --- Main Entry Point ---
if __name__ == "__main__":
    print("‚ñ∂Ô∏è Extracting frames...")
    extract_frames(VIDEO_PATH)

    print("üéß Extracting and transcribing audio...")
    extract_audio(VIDEO_PATH, AUDIO_FILE)
    transcript = transcribe_audio(AUDIO_FILE)

    print("üß† Generating visual embeddings...")
    frame_embeddings = embed_frames()

    print("üìù Generating transcript embedding...")
    transcript_embedding = get_text_embedding(transcript)

    print("‚úÖ Done! You now have frame-level visual embeddings and a transcript embedding ready for search or reasoning.")

```
