# ğŸ§  Video Keyframe and Audio Embedding Pipeline

This project provides a full pipeline to:

- Extract keyframes from a video
- Embed those frames using the CLIP model
- Transcribe audio using OpenAI Whisper
- Embed the transcribed audio using OpenAI Embeddings API

---

## ğŸ“¦ Features

- âœ… Extracts frames at fixed time intervals from any video
- âœ… Embeds image frames using HuggingFace `CLIP`
- âœ… Transcribes audio tracks using OpenAI `whisper`
- âœ… Embeds resulting transcript text using OpenAI's `text-embedding-3-small`
- ğŸ§ª Optional: Upload frame data to S3 (placeholder)

---

## ğŸ› ï¸ Tech Stack

- `Python 3.11.9`
- `Whisper` (OpenAI)
- `Transformers` (CLIP from Hugging Face)
- `Torch` for tensor operations
- `OpenAI SDK` (>= 1.0)
- `PIL`, `cv2` for image handling
- `dotenv` for secure key management

---

## ğŸ“‚ Project Structure

---

## âš™ï¸ Setup

### 1. Clone and create a virtual environment

```bash
git clone <repo-url>
cd video_embedding_process
pyenv local 3.11.9
python -m venv venv
source venv/bin/activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up your .env file

```bash
OPENAI_API_KEY=your_openai_key
```

### â–¶ï¸ Run the pipeline

```bash
python src/main.py
```

Youâ€™ll be prompted:

```bash
>video y/n?
>audio y/n?
```

Select y to process the video and/or transcribe + embed audio

### Â âœï¸ Notes

- Whisper models available: "tiny", "base", "small", "medium", "large", "turbo" (if using OpenAI hosted)
- Frame extraction interval and input paths are defined in constants.py
- GPU acceleration is supported with Whisper + Torch

### ğŸ“Œ Future Improvements

- Upload to AWS S3 or other cloud storage
- Store embeddings in a vector database (like Pinecone or Weaviate)
- Parallelize frame and audio processing for speed
